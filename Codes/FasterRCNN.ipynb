{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "[{'boxes': tensor([[705.5526, 469.8596, 783.2929, 507.4194],\n",
      "        [637.0988, 423.2969, 698.1732, 471.6514],\n",
      "        [419.4441, 430.9796, 495.7361, 477.3641],\n",
      "        [561.7086, 388.9178, 628.7439, 430.0663],\n",
      "        [356.1621, 310.7462, 416.5035, 342.5398],\n",
      "        [593.6676, 398.3850, 660.3770, 451.2029],\n",
      "        [376.4500, 364.3586, 443.5538, 404.2065],\n",
      "        [311.6594, 151.2979, 346.3241, 166.1227],\n",
      "        [463.0050, 259.5233, 512.2544, 289.8143],\n",
      "        [370.5443, 350.6823, 422.2513, 378.2713],\n",
      "        [ 87.2541, 106.6691,  97.0644, 116.7646],\n",
      "        [322.6376, 214.9021, 369.6575, 237.8207],\n",
      "        [400.1416, 410.9363, 467.9415, 454.6382],\n",
      "        [ 85.2791, 126.1788, 101.3257, 142.6267],\n",
      "        [102.7473,  81.8878, 115.4005,  96.3613],\n",
      "        [390.3857, 390.7924, 459.0039, 433.8890],\n",
      "        [ 68.9947, 206.3729,  96.9899, 231.5285],\n",
      "        [176.1313, 228.1316, 185.8716, 248.9348],\n",
      "        [539.6714, 369.3660, 599.8673, 406.4590],\n",
      "        [ 48.5398, 241.7710,  73.3752, 272.7346],\n",
      "        [262.5395, 391.8955, 285.6181, 418.0518],\n",
      "        [360.6093, 143.5049, 393.1770, 161.3970],\n",
      "        [168.8655, 112.1004, 185.1367, 127.3240],\n",
      "        [489.4193, 287.0543, 542.7299, 319.2196],\n",
      "        [ 52.9487,  34.9256,  63.9815,  44.7128],\n",
      "        [308.3147, 179.5391, 347.8870, 198.6071],\n",
      "        [384.4468, 165.8233, 409.6582, 187.4932],\n",
      "        [459.0090, 250.3638, 506.0078, 280.5558],\n",
      "        [109.9916,  29.3092, 121.3206,  40.7664],\n",
      "        [335.4530, 229.9331, 383.1746, 259.1656],\n",
      "        [450.9544, 242.6871, 496.0863, 271.4734],\n",
      "        [344.7793, 164.2146, 380.7339, 180.6280],\n",
      "        [398.3546, 191.9781, 431.6320, 208.6463],\n",
      "        [445.2234, 234.7690, 490.7878, 265.0754],\n",
      "        [116.9212, 202.2787, 126.5321, 218.9612],\n",
      "        [312.0529, 187.1348, 362.1385, 225.0362],\n",
      "        [513.0884, 330.0426, 580.9452, 368.6944],\n",
      "        [193.1000, 234.2798, 202.0825, 251.9668],\n",
      "        [439.5267, 215.7076, 478.6172, 235.9167],\n",
      "        [232.8493, 228.2646, 242.4393, 250.2613],\n",
      "        [707.9121,  91.2286, 726.0360, 105.1070],\n",
      "        [580.3303, 393.6713, 640.8474, 441.7443],\n",
      "        [308.6192, 181.7783, 359.8500, 213.9829],\n",
      "        [456.9433, 240.7725, 490.9071, 255.2746],\n",
      "        [ 89.7965,  79.8360,  98.5118,  92.5389],\n",
      "        [109.5097, 100.6931, 116.5686, 118.0602],\n",
      "        [340.1000, 259.6299, 398.4110, 305.2927],\n",
      "        [418.3818, 201.8137, 467.0652, 223.1805],\n",
      "        [135.5493, 114.4463, 143.8444, 127.4842],\n",
      "        [ 84.7406, 193.4560,  94.8993, 214.4630],\n",
      "        [441.6784, 224.8677, 486.2221, 249.1439],\n",
      "        [336.4230, 249.1346, 405.0266, 313.9509],\n",
      "        [731.8340,  81.0928, 751.7506,  94.0616],\n",
      "        [ 32.2120, 278.6222,  64.6030, 311.4128],\n",
      "        [130.7910,  63.6150, 136.2594,  76.4771],\n",
      "        [550.5633, 378.1996, 614.1185, 421.4112],\n",
      "        [248.5300, 303.3544, 266.0682, 328.8961],\n",
      "        [ 31.2162,  37.3170,  39.5713,  44.9901],\n",
      "        [396.4926, 421.4691, 436.6914, 458.8255],\n",
      "        [498.4099, 303.4202, 560.5908, 350.8283],\n",
      "        [433.9572, 208.7770, 480.2617, 231.2984],\n",
      "        [812.1815,  83.6696, 817.2991,  96.6582],\n",
      "        [278.5210,  12.7010, 294.3882,  26.8796],\n",
      "        [433.9846, 203.3243, 464.0315, 218.7237],\n",
      "        [453.6622, 215.1469, 481.5995, 229.6377],\n",
      "        [337.4532, 240.3423, 388.5371, 271.4629],\n",
      "        [ 86.8879, 197.6189,  98.6594, 220.1617],\n",
      "        [ 41.2360,  37.2193,  51.3204,  46.2459],\n",
      "        [354.5795, 293.4078, 416.2585, 320.5987],\n",
      "        [339.9949, 251.7940, 394.7519, 285.5379],\n",
      "        [491.0045, 289.4846, 554.2398, 337.4356],\n",
      "        [129.1721,  49.5836, 136.8168,  61.2493],\n",
      "        [908.0452,  78.2560, 912.3974,  90.6130],\n",
      "        [582.7817, 121.3834, 593.1351, 135.3963],\n",
      "        [116.6292, 199.9193, 125.3331, 217.5510],\n",
      "        [104.9707,  47.1186, 112.5708,  53.5847],\n",
      "        [490.0995, 290.1194, 560.5776, 347.1628],\n",
      "        [895.3798,  70.2825, 900.7111,  82.1966],\n",
      "        [501.8053, 307.1615, 578.7720, 362.0743],\n",
      "        [511.4680, 327.8497, 582.4077, 368.5758],\n",
      "        [442.2877, 153.9111, 449.9290, 164.7171],\n",
      "        [346.1694, 269.9311, 410.4215, 315.3416],\n",
      "        [248.5277, 302.5963, 266.1712, 329.2849],\n",
      "        [347.3910, 274.8269, 411.5136, 316.4230],\n",
      "        [ 19.7698,  41.4956,  27.9372,  53.3888],\n",
      "        [368.2745, 347.2650, 440.9725, 396.2785],\n",
      "        [338.9500, 249.8327, 395.7293, 286.6095],\n",
      "        [ 71.9258, 199.4106,  90.4604, 225.1705],\n",
      "        [ 90.4660,  42.7669,  95.8803,  50.5493],\n",
      "        [897.3065,  70.6950, 902.3843,  82.6300],\n",
      "        [315.8080, 201.8483, 364.4743, 230.5729],\n",
      "        [ 16.2074,  33.9978,  25.3438,  49.4466],\n",
      "        [109.3504, 104.9694, 116.6442, 118.7132],\n",
      "        [109.6446,  96.5732, 116.9596, 111.4748],\n",
      "        [326.4636, 220.0600, 375.5186, 247.6779],\n",
      "        [348.8198, 283.0422, 416.1468, 335.4693],\n",
      "        [ 88.6254, 205.1935,  99.4354, 226.0935],\n",
      "        [ 38.2430,  37.2745,  48.1853,  45.4907],\n",
      "        [135.1528,  53.0004, 143.6167,  61.0753],\n",
      "        [137.2015,  51.7943, 145.7317,  60.2898]]), 'labels': tensor([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  1,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  8,\n",
      "         3,  1,  3,  1,  3,  3,  3,  3,  3,  1,  3,  3,  3,  1,  3,  8,  3,  3,\n",
      "         1,  3,  3,  3,  3,  3,  3,  1, 10,  3,  3,  3,  1,  3,  3,  3,  3,  1,\n",
      "         1,  1,  1,  3,  8,  1,  8,  8,  1,  3,  8,  8,  1,  3,  8,  1,  1,  1,\n",
      "         3,  1,  4,  1,  3,  3,  1,  3,  3,  3]), 'scores': tensor([0.9930, 0.9856, 0.9832, 0.9784, 0.9688, 0.9673, 0.9528, 0.9399, 0.9201,\n",
      "        0.9195, 0.9188, 0.9146, 0.9094, 0.9076, 0.9019, 0.8993, 0.8986, 0.8968,\n",
      "        0.8911, 0.8898, 0.8807, 0.8736, 0.8636, 0.8439, 0.8359, 0.8311, 0.8120,\n",
      "        0.8078, 0.7989, 0.7873, 0.7612, 0.7509, 0.7483, 0.7461, 0.7092, 0.7052,\n",
      "        0.7009, 0.6919, 0.6823, 0.6748, 0.6620, 0.6554, 0.6113, 0.6085, 0.5947,\n",
      "        0.5855, 0.5721, 0.5666, 0.5664, 0.5647, 0.5540, 0.5427, 0.5349, 0.5267,\n",
      "        0.5088, 0.4963, 0.4790, 0.4788, 0.4786, 0.4692, 0.4607, 0.4551, 0.4418,\n",
      "        0.4349, 0.4179, 0.3928, 0.3853, 0.3502, 0.3433, 0.3430, 0.3382, 0.3267,\n",
      "        0.3260, 0.3086, 0.2950, 0.2716, 0.2671, 0.2662, 0.2477, 0.2475, 0.2474,\n",
      "        0.2351, 0.2138, 0.2106, 0.1910, 0.1909, 0.1780, 0.1744, 0.1679, 0.1491,\n",
      "        0.1490, 0.1476, 0.1450, 0.1430, 0.1387, 0.1370, 0.1331, 0.1313, 0.1263,\n",
      "        0.1262])}]\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "7\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "7\n",
      "2\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "torch.save(model.state_dict(), 'pretrained_fasterrcnn_resnet50_fpn.pth')\n",
    "\n",
    "\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)  # Make sure to set pretrained to False\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('pretrained_fasterrcnn_resnet50_fpn.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',    'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "print(len(classes))\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "path = \"data/visdrone/VisDrone2019-DET-train/images/0000002_00005_d_0000014.jpg\"\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(path)\n",
    "\n",
    "img_name  = path.split('/')[-1]\n",
    "\n",
    "# Apply the transformation\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Add a batch dimension\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Threshold for filtering detections\n",
    "threshold = 0.5\n",
    "print(predictions)\n",
    "# Display the predictions\n",
    "for i in range(len(predictions[0]['boxes'])):\n",
    "    score = predictions[0]['scores'][i]\n",
    "    if score > threshold:\n",
    "        box = predictions[0]['boxes'][i]\n",
    "        class_id = predictions[0]['labels'][i].item() - 1  # Subtract 1 because COCO labels start from 1\n",
    "        print(class_id)\n",
    "        try:\n",
    "            class_name = classes[class_id]\n",
    "\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
    "            draw.text((box[0], box[1]), f\"{class_name.capitalize()} - score: {score:.2f}\", fill=\"blue\")\n",
    "        except:\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
    "            draw.text((box[0], box[1]), f\"{class_id} - score: {score:.2f}\", fill=\"blue\")\n",
    "\n",
    "        \n",
    "\n",
    "# Display the image\n",
    "image.show()\n",
    "\n",
    "# Save results\n",
    "try:\n",
    "    os.mkdir(\"results/faster_rcnn\")\n",
    "except:\n",
    "    pass\n",
    "image.save(f\"results/faster_rcnn/FRCNN_{img_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing : Accuracy and mAP Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "perc = 1\n",
    "\n",
    "# Function to calculate accuracy for an image and its annotation\n",
    "def calculate_accuracy(image_path, annotation_path,perc):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Apply the transformation\n",
    "    image_tensor = transform(image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    # Read ground truth annotations from the text file\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        ground_truth_lines = file.readlines()\n",
    "\n",
    "    # Parse ground truth bounding boxes\n",
    "    ground_truth_boxes = []\n",
    "    for line in ground_truth_lines:\n",
    "        parts = line.strip().split(\",\")\n",
    "        box = [int(parts[0]), int(parts[1]), int(parts[0]) + int(parts[2]), int(parts[1]) + int(parts[3])]\n",
    "        ground_truth_boxes.append(box)\n",
    "\n",
    "    # Initialize variables for counting true positives and total predictions\n",
    "    true_positives = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Calculate accuracy for each predicted bounding box\n",
    "    for i in range(len(predictions[0]['boxes'])):\n",
    "        score = predictions[0]['scores'][i]\n",
    "        if score > threshold:\n",
    "            total_predictions += 1\n",
    "            pred_box = predictions[0]['boxes'][i]\n",
    "            pred_box = [pred_box[0].item(), pred_box[1].item(), pred_box[2].item(), pred_box[3].item()]\n",
    "\n",
    "            # Calculate IoU with ground truth boxes\n",
    "            iou_scores = []\n",
    "            for gt_box in ground_truth_boxes:\n",
    "                x1 = max(pred_box[0], gt_box[0])\n",
    "                y1 = max(pred_box[1], gt_box[1])\n",
    "                x2 = min(pred_box[2], gt_box[2])\n",
    "                y2 = min(pred_box[3], gt_box[3])\n",
    "\n",
    "                intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "                area_pred = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "                area_gt = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "                union = area_pred + area_gt - intersection\n",
    "\n",
    "                iou = intersection / union\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "            # Check if IoU meets threshold for any ground truth box\n",
    "            if max(iou_scores) >= perc:  # Assuming IoU threshold of 0.5 for correct detection\n",
    "                true_positives += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = true_positives / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Folder paths\n",
    "images_folder = \"data/visdrone/VisDrone2019-DET-val/images\"\n",
    "annotations_folder = \"data/visdrone/VisDrone2019-DET-val/annotations\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Faster RCNN========\n",
      "||                       ||\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "def calculate_mAP(accuracy_values):\n",
    "    # Sort accuracy values in descending order\n",
    "    sorted_accuracy = sorted(accuracy_values, reverse=True)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = []\n",
    "    recall = []\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    total_positives = len(sorted_accuracy)\n",
    "\n",
    "    for acc in sorted_accuracy:\n",
    "        true_positives += acc\n",
    "        false_positives += 1 - acc\n",
    "        precision.append(true_positives / (true_positives + false_positives))\n",
    "        recall.append(true_positives / total_positives)\n",
    "\n",
    "    # Calculate Average Precision (AP) using precision-recall curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(sorted_accuracy)):\n",
    "        ap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "    print(f\"||  AP%{int(perc*100)} Score = {ap*100:.2f}  ||\")\n",
    "    # Calculate mean Average Precision (mAP)\n",
    "    mAP = ap / len(sorted_accuracy)\n",
    "    return mAP\n",
    "\n",
    "\n",
    "\n",
    "accuracy_values = []\n",
    "iter = 100\n",
    "\n",
    "x = len(os.listdir(images_folder))\n",
    "\n",
    "perc_vals = []\n",
    "a = 0.5\n",
    "for i in range(0,10):\n",
    "    perc_vals.append(round(a,2))\n",
    "    a+=0.05\n",
    "perc_vals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# image_list =['']*iter\n",
    "\n",
    "# for ind in range(iter):\n",
    "#     y = random.randint(0, len(os.listdir(images_folder))-1)\n",
    "#     image_name = os.listdir(images_folder)[y]\n",
    "#     image_list[ind] = image_name\n",
    "\n",
    "# print(image_list)\n",
    "image_list = ['0000194_00625_d_0000122.jpg', '0000364_01373_d_0000780.jpg', '0000026_02500_d_0000029.jpg', '0000333_03137_d_0000017.jpg', '0000327_01001_d_0000716.jpg', '0000023_00000_d_0000008.jpg', '0000153_00001_d_0000001.jpg', '0000316_00001_d_0000519.jpg', '0000271_04401_d_0000394.jpg', '0000356_05097_d_0000655.jpg', '0000289_03201_d_0000827.jpg', '0000271_05001_d_0000397.jpg', '0000287_00201_d_0000760.jpg', '0000280_00001_d_0000612.jpg', '0000215_02667_d_0000262.jpg', '0000116_01059_d_0000085.jpg', '0000333_02941_d_0000016.jpg', '0000116_00819_d_0000084.jpg', '0000359_03529_d_0000712.jpg', '0000316_00401_d_0000521.jpg', '0000194_00200_d_0000120.jpg', '0000154_02001_d_0000001.jpg', '0000280_02601_d_0000625.jpg', '0000271_06601_d_0000405.jpg', '0000356_04901_d_0000654.jpg', '0000271_00401_d_0000376.jpg', '0000287_01201_d_0000765.jpg', '0000069_00001_d_0000001.jpg', '0000330_04601_d_0000823.jpg', '0000289_04601_d_0000834.jpg', '0000289_06401_d_0000843.jpg', '0000364_00589_d_0000798.jpg', '0000242_00627_d_0000003.jpg', '0000289_00401_d_0000813.jpg', '0000289_05401_d_0000838.jpg', '0000330_01401_d_0000807.jpg', '0000242_00500_d_0000002.jpg', '0000280_03001_d_0000627.jpg', '0000360_07057_d_0000749.jpg', '0000346_03921_d_0000366.jpg', '0000359_03529_d_0000712.jpg', '0000271_04801_d_0000396.jpg', '0000001_05499_d_0000010.jpg', '0000244_00001_d_0000001.jpg', '0000348_03333_d_0000423.jpg', '0000244_04400_d_0000010.jpg', '0000308_04401_d_0000327.jpg', '0000069_00001_d_0000001.jpg', '0000295_01400_d_0000028.jpg', '0000277_01401_d_0000546.jpg', '0000026_04500_d_0000033.jpg', '0000346_07057_d_0000382.jpg', '0000277_01401_d_0000546.jpg', '0000249_00001_d_0000001.jpg', '0000069_01878_d_0000005.jpg', '0000271_07001_d_0000407.jpg', '0000290_04001_d_0000867.jpg', '0000069_02480_d_0000007.jpg', '0000213_05745_d_0000247.jpg', '0000359_03333_d_0000711.jpg', '0000330_04601_d_0000823.jpg', '0000327_03801_d_0000730.jpg', '0000116_01059_d_0000085.jpg', '0000155_00401_d_0000001.jpg', '0000103_04513_d_0000034.jpg', '0000244_05900_d_0000013.jpg', '0000213_02500_d_0000240.jpg', '0000086_01954_d_0000005.jpg', '0000276_05201_d_0000533.jpg', '0000024_01543_d_0000015.jpg', '0000356_02745_d_0000643.jpg', '0000116_00351_d_0000083.jpg', '0000215_00000_d_0000256.jpg', '0000291_01401_d_0000875.jpg', '0000280_01001_d_0000617.jpg', '0000333_02941_d_0000016.jpg', '0000291_05201_d_0000893.jpg', '0000333_03529_d_0000019.jpg', '0000359_01961_d_0000707.jpg', '0000242_01139_d_0000006.jpg', '0000335_03921_d_0000063.jpg', '0000276_03801_d_0000526.jpg', '0000153_00401_d_0000001.jpg', '0000116_01059_d_0000085.jpg', '0000335_03137_d_0000059.jpg', '0000271_07001_d_0000407.jpg', '0000289_03201_d_0000827.jpg', '0000276_01401_d_0000514.jpg', '0000001_03499_d_0000006.jpg', '0000024_01000_d_0000014.jpg', '0000213_05340_d_0000246.jpg', '0000069_02163_d_0000006.jpg', '0000277_04201_d_0000559.jpg', '0000276_01401_d_0000514.jpg', '0000276_02601_d_0000520.jpg', '0000244_00500_d_0000002.jpg', '0000291_01401_d_0000875.jpg', '0000335_02745_d_0000057.jpg', '0000289_06201_d_0000842.jpg', '0000316_00201_d_0000520.jpg']\n",
    "\n",
    "\n",
    "print(\"Faster RCNN\".center(27,'='))\n",
    "print(\"||                       ||\")\n",
    "\n",
    "\n",
    "for perc in perc_vals:\n",
    "    for ind in range(iter):\n",
    "        \n",
    "        image_name = image_list[ind]\n",
    "        if image_name.endswith('.jpg'):\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            annotation_name = image_name.split('.')[0] + '.txt'\n",
    "            annotation_path = os.path.join(annotations_folder, annotation_name)\n",
    "\n",
    "            accuracy = calculate_accuracy(image_path, annotation_path,perc)\n",
    "            \n",
    "            accuracy_values.append(round(accuracy,4))\n",
    "            \n",
    "\n",
    "    import math\n",
    "    # Calculate mAP\n",
    "    # mAP = calculate_mAP(accuracy_values)\n",
    "    # print(f\"mAP value = : {mAP*100:.4f} \")\n",
    "    # print(f\"Max Accuracy  = : {max(accuracy_values)*100:.4f} \")\n",
    "    # print(f\"Min Accuracy  = : {min(accuracy_values)*100:.4f} \")\n",
    "    print(f\"Average Accuracy  = : {(sum(accuracy_values)/len(accuracy_values))*100:.4f} \")\n",
    "    # print()\n",
    "    # print(accuracy_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP%50 \n",
    "\n",
    "AP%75 \n",
    "\n",
    "AP%95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
