{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nish Parikh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nish Parikh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"data/visdrone/VisDrone2019-DET-train/images/0000002_00005_d_0000014.jpg\"\n",
    "img_name = path.split('/')[-1]\n",
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',    'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# Define the model\n",
    "model = retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "torch.save(model.state_dict(), 'pretrained_retinanet_resnet50_fpn.pth')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(path)\n",
    "\n",
    "# Apply the transformation\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Add a batch dimension\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Threshold for filtering detections\n",
    "threshold = 0.5\n",
    "\n",
    "# Display the predictions\n",
    "print(len(predictions[0]['boxes']))\n",
    "for i in range(len(predictions[0]['boxes'])):\n",
    "    score = predictions[0]['scores'][i]\n",
    "    if score > threshold:\n",
    "        box = predictions[0]['boxes'][i]\n",
    "        class_id = predictions[0]['labels'][i].item()\n",
    "        class_name = str(class_id)  # In RetinaNet, class labels are numeric IDs\n",
    "\n",
    "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
    "        draw.text((box[0], box[1]), f\"{classes[int(class_name)-1]} - score: {score:.2f}\", fill=\"red\")\n",
    "\n",
    "# Display the image\n",
    "image.show()\n",
    "\n",
    "# Save results\n",
    "try:\n",
    "    os.mkdir(\"results/retinaNet\")\n",
    "except:\n",
    "    pass\n",
    "image.save(f\"results/retinaNet/Retina_{img_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "perc = 1\n",
    "\n",
    "\n",
    "# Function to calculate accuracy for an image and its annotation\n",
    "def calculate_accuracy(image_path, annotation_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Apply the transformation\n",
    "    image_tensor = transform(image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    # Read ground truth annotations from the text file\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        ground_truth_lines = file.readlines()\n",
    "\n",
    "    # Parse ground truth bounding boxes\n",
    "    ground_truth_boxes = []\n",
    "    for line in ground_truth_lines:\n",
    "        parts = line.strip().split(\",\")\n",
    "        box = [int(parts[0]), int(parts[1]), int(parts[0]) + int(parts[2]), int(parts[1]) + int(parts[3])]\n",
    "        ground_truth_boxes.append(box)\n",
    "\n",
    "    # Initialize variables for counting true positives and total predictions\n",
    "    true_positives = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Calculate accuracy for each predicted bounding box\n",
    "    for i in range(len(predictions[0]['boxes'])):\n",
    "        score = predictions[0]['scores'][i]\n",
    "        if score > threshold:\n",
    "            total_predictions += 1\n",
    "            pred_box = predictions[0]['boxes'][i]\n",
    "            pred_box = [pred_box[0].item(), pred_box[1].item(), pred_box[2].item(), pred_box[3].item()]\n",
    "\n",
    "            # Calculate IoU with ground truth boxes\n",
    "            iou_scores = []\n",
    "            for gt_box in ground_truth_boxes:\n",
    "                x1 = max(pred_box[0], gt_box[0])\n",
    "                y1 = max(pred_box[1], gt_box[1])\n",
    "                x2 = min(pred_box[2], gt_box[2])\n",
    "                y2 = min(pred_box[3], gt_box[3])\n",
    "\n",
    "                intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "                area_pred = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "                area_gt = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "                union = area_pred + area_gt - intersection\n",
    "\n",
    "                iou = intersection / union\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "            # Check if IoU meets threshold for any ground truth box\n",
    "            if max(iou_scores) >= perc:  # Assuming IoU threshold of 0.5 for correct detection\n",
    "                true_positives += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = true_positives / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Folder paths\n",
    "images_folder = \"data/visdrone/VisDrone2019-DET-val/images\"\n",
    "annotations_folder = \"data/visdrone/VisDrone2019-DET-val/annotations\"\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "def calculate_mAP(accuracy_values):\n",
    "    # Sort accuracy values in descending order\n",
    "    sorted_accuracy = sorted(accuracy_values, reverse=True)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = []\n",
    "    recall = []\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    total_positives = len(sorted_accuracy)\n",
    "\n",
    "    for acc in sorted_accuracy:\n",
    "        true_positives += acc\n",
    "        false_positives += 1 - acc\n",
    "        precision.append(true_positives / (true_positives + false_positives))\n",
    "        recall.append(true_positives / total_positives)\n",
    "\n",
    "    # Calculate Average Precision (AP) using precision-recall curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(sorted_accuracy)):\n",
    "        ap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "    print(f\"||  AP%{int(perc*100)} Score = {(ap/2)*100:.2f}  ||\".center(27,' '))\n",
    "    print(\"||                       ||\")\n",
    "\n",
    "    # Calculate mean Average Precision (mAP)\n",
    "    mAP = ap / len(sorted_accuracy)\n",
    "    return mAP\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========RetinaNet=========\n",
      "||                       ||\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_values = []\n",
    "iter = 100\n",
    "\n",
    "image_list = ['0000194_00625_d_0000122.jpg', '0000364_01373_d_0000780.jpg', '0000026_02500_d_0000029.jpg', '0000333_03137_d_0000017.jpg', '0000327_01001_d_0000716.jpg', '0000023_00000_d_0000008.jpg', '0000153_00001_d_0000001.jpg', '0000316_00001_d_0000519.jpg', '0000271_04401_d_0000394.jpg', '0000356_05097_d_0000655.jpg', '0000289_03201_d_0000827.jpg', '0000271_05001_d_0000397.jpg', '0000287_00201_d_0000760.jpg', '0000280_00001_d_0000612.jpg', '0000215_02667_d_0000262.jpg', '0000116_01059_d_0000085.jpg', '0000333_02941_d_0000016.jpg', '0000116_00819_d_0000084.jpg', '0000359_03529_d_0000712.jpg', '0000316_00401_d_0000521.jpg', '0000194_00200_d_0000120.jpg', '0000154_02001_d_0000001.jpg', '0000280_02601_d_0000625.jpg', '0000271_06601_d_0000405.jpg', '0000356_04901_d_0000654.jpg', '0000271_00401_d_0000376.jpg', '0000287_01201_d_0000765.jpg', '0000069_00001_d_0000001.jpg', '0000330_04601_d_0000823.jpg', '0000289_04601_d_0000834.jpg', '0000289_06401_d_0000843.jpg', '0000364_00589_d_0000798.jpg', '0000242_00627_d_0000003.jpg', '0000289_00401_d_0000813.jpg', '0000289_05401_d_0000838.jpg', '0000330_01401_d_0000807.jpg', '0000242_00500_d_0000002.jpg', '0000280_03001_d_0000627.jpg', '0000360_07057_d_0000749.jpg', '0000346_03921_d_0000366.jpg', '0000359_03529_d_0000712.jpg', '0000271_04801_d_0000396.jpg', '0000001_05499_d_0000010.jpg', '0000244_00001_d_0000001.jpg', '0000348_03333_d_0000423.jpg', '0000244_04400_d_0000010.jpg', '0000308_04401_d_0000327.jpg', '0000069_00001_d_0000001.jpg', '0000295_01400_d_0000028.jpg', '0000277_01401_d_0000546.jpg', '0000026_04500_d_0000033.jpg', '0000346_07057_d_0000382.jpg', '0000277_01401_d_0000546.jpg', '0000249_00001_d_0000001.jpg', '0000069_01878_d_0000005.jpg', '0000271_07001_d_0000407.jpg', '0000290_04001_d_0000867.jpg', '0000069_02480_d_0000007.jpg', '0000213_05745_d_0000247.jpg', '0000359_03333_d_0000711.jpg', '0000330_04601_d_0000823.jpg', '0000327_03801_d_0000730.jpg', '0000116_01059_d_0000085.jpg', '0000155_00401_d_0000001.jpg', '0000103_04513_d_0000034.jpg', '0000244_05900_d_0000013.jpg', '0000213_02500_d_0000240.jpg', '0000086_01954_d_0000005.jpg', '0000276_05201_d_0000533.jpg', '0000024_01543_d_0000015.jpg', '0000356_02745_d_0000643.jpg', '0000116_00351_d_0000083.jpg', '0000215_00000_d_0000256.jpg', '0000291_01401_d_0000875.jpg', '0000280_01001_d_0000617.jpg', '0000333_02941_d_0000016.jpg', '0000291_05201_d_0000893.jpg', '0000333_03529_d_0000019.jpg', '0000359_01961_d_0000707.jpg', '0000242_01139_d_0000006.jpg', '0000335_03921_d_0000063.jpg', '0000276_03801_d_0000526.jpg', '0000153_00401_d_0000001.jpg', '0000116_01059_d_0000085.jpg', '0000335_03137_d_0000059.jpg', '0000271_07001_d_0000407.jpg', '0000289_03201_d_0000827.jpg', '0000276_01401_d_0000514.jpg', '0000001_03499_d_0000006.jpg', '0000024_01000_d_0000014.jpg', '0000213_05340_d_0000246.jpg', '0000069_02163_d_0000006.jpg', '0000277_04201_d_0000559.jpg', '0000276_01401_d_0000514.jpg', '0000276_02601_d_0000520.jpg', '0000244_00500_d_0000002.jpg', '0000291_01401_d_0000875.jpg', '0000335_02745_d_0000057.jpg', '0000289_06201_d_0000842.jpg', '0000316_00201_d_0000520.jpg']\n",
    "\n",
    "\n",
    "perc_vals = []\n",
    "a = 0.5\n",
    "for i in range(0,10):\n",
    "    perc_vals.append(round(a,2))\n",
    "    a+=0.05\n",
    "\n",
    "\n",
    "\n",
    "x = len(os.listdir(images_folder))\n",
    "\n",
    "print(\"RetinaNet\".center(27,'='))\n",
    "print(\"||                       ||\")\n",
    "for perc in perc_vals:\n",
    "\n",
    "    for ind in tqdm(range(iter)):\n",
    "        # y = random.randint(0, len(os.listdir(images_folder))-1)\n",
    "        # image_name = os.listdir(images_folder)[y]\n",
    "        image_name = image_list[ind]\n",
    "        if image_name.endswith('.jpg'):\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            # print()\n",
    "            # print(images_folder+'/'+image_name)\n",
    "\n",
    "            annotation_name = image_name.split('.')[0] + '.txt'\n",
    "            annotation_path = os.path.join(annotations_folder, annotation_name)\n",
    "\n",
    "            accuracy = calculate_accuracy(image_path, annotation_path)\n",
    "            \n",
    "            accuracy_values.append(round(accuracy,4))\n",
    "            \n",
    "\n",
    "    import math\n",
    "    # Calculate mAP\n",
    "    # mAP = calculate_mAP(accuracy_values)\n",
    "    # print(f\"mAP value = : {mAP*100:.4f} \")\n",
    "    # print(f\"Max Accuracy  = : {max(accuracy_values)*100:.4f} \")\n",
    "    # print(f\"Min Accuracy  = : {min(accuracy_values)*100:.4f} \")\n",
    "    print(f\"Average Accuracy  = : {(sum(accuracy_values)/len(accuracy_values))*100:.4f} \")\n",
    "    print()\n",
    "    break\n",
    "    # print(accuracy_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
